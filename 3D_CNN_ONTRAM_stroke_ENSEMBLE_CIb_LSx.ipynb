{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIb LSx: ONTRAM 3D CNN\n",
    "## Outcome: stroke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import ndimage\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Tensorflow/Keras\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "print(keras.__version__)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Own functions\n",
    "from functions.plot_slices import plot_slices\n",
    "from functions.ontram import ontram\n",
    "from functions.fit_ontram import fit_ontram\n",
    "from functions.fit_ontram_batches import fit_ontram_batches\n",
    "from functions.plot_results import plot_results\n",
    "from functions.methods import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_VARIABLE = \"stroke\"\n",
    "# OUTPUT_VARIABLE = \"mrs\"\n",
    "N_ENSEMBLES = 5\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"/tf/notebooks/katrin/\"\n",
    "OUTPUT_DIR = '{}results/stroke/ensemble/'.format(DIR)\n",
    "MODEL_DIR = '{}results/stroke/ensemble/CIb/'.format(DIR)\n",
    "INPUT_IMG = \"{}data/dicom_3d_128x128x30.h5\".format(DIR)\n",
    "INPUT_TAB = \"{}data/baseline_data_DWI_imputed.csv\".format(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_data(string):\n",
    "    decoded_string = [n.decode(\"UTF-8\", \"ignore\") for n in string]\n",
    "    return(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(INPUT_IMG, \"r\") as h5:\n",
    "    print(h5.keys())\n",
    "    X = h5[\"X\"][:]\n",
    "    Y_pat = h5[\"stroke\"][:]\n",
    "    Y_img = h5[\"Y\"][:]\n",
    "    pat = decode_data(h5[\"pat\"])[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, Y_pat.shape, Y_img.shape, len(pat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape, X.min(), X.max(), X.mean(), X.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- standardize each patient to 0 mean, 1 variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(array):\n",
    "    mean = np.mean(array)\n",
    "    sd = np.std(array)\n",
    "    standardized = (array - mean) / sd\n",
    "    return standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([standardize(x) for x in X])\n",
    "X = np.expand_dims(X, axis = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, X.min(), X.max(), X.mean(), X.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(INPUT_TAB, sep = ',')\n",
    "dat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change values to numbers\n",
    "dat = dat.replace('no', 0)\n",
    "dat = dat.replace('yes', 1)\n",
    "dat.sex = dat.sex.replace('female', 1)\n",
    "dat.sex = dat.sex.replace('male', 0)\n",
    "dat.event = dat.event.replace('Stroke', 1)\n",
    "dat.event = dat.event.replace('TIA', 0)\n",
    "dat.p_id =[format(id, '03d') for id in dat.p_id]\n",
    "dat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables we have\n",
    "dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mRS binary \n",
    "dat[\"mrs_3months_binary\"] = 1\n",
    "dat.loc[dat.mrs_3months <= 2, \"mrs_3months_binary\"] = 0\n",
    "plt.hist(dat.event, bins = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match tabular data to image data\n",
    "X_tab = np.zeros((X.shape[0], 12))\n",
    "Y_mrs = np.zeros((X.shape[0]))\n",
    "Y_pat = np.zeros((dat.shape[0]))\n",
    "for i, p in enumerate(pat):\n",
    "    k = np.where(dat.p_id.values == p)[0][0]\n",
    "    dat_tmp = dat.iloc[k]\n",
    "    X_tab[i,:] = np.array([dat_tmp.age, dat_tmp.sex, dat_tmp.mrs_before, dat_tmp.nihss_baseline, \n",
    "                           dat_tmp.stroke_before, dat_tmp.tia_before, dat_tmp.rf_hypertonia, \n",
    "                           dat_tmp.rf_diabetes, dat_tmp.rf_hypercholesterolemia, dat_tmp.rf_smoker, \n",
    "                           dat_tmp.rf_atrial_fibrillation, dat_tmp.rf_chd])\n",
    "    Y_mrs[i] = dat_tmp.mrs_3months_binary\n",
    "    Y_pat[i] = dat_tmp.event\n",
    "X_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_VARIABLE == \"stroke\":\n",
    "    Y = Y_pat\n",
    "elif OUTPUT_VARIABLE == \"mrs\":\n",
    "    Y = Y_mrs\n",
    "else:\n",
    "    raise ValueError(\"unknown OUTPUT_VARIABLE: {}\".format(OUTPUT_VARIABLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = 10\n",
    "plot_slices(X[j], pat, \"axial\", modality = \"DWI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zoom\n",
    "def random_zoom3d(X_im, min_zoom, max_zoom):\n",
    "    z = np.random.sample() *(max_zoom-min_zoom) + min_zoom\n",
    "    zoom_matrix = np.array([[z, 0, 0, 0],\n",
    "                            [0, z, 0, 0],\n",
    "                            [0, 0, z, 0],\n",
    "                            [0, 0, 0, 1]])\n",
    "    return ndimage.affine_transform(X_im, zoom_matrix, mode = \"nearest\", order = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rotate\n",
    "def random_rotate3d(X_im, min_angle_xy, max_angle_xy, min_angle_xz, max_angle_xz, min_angle_yz, max_angle_yz):\n",
    "    angle_xy = np.random.uniform(min_angle_xy, max_angle_xy)\n",
    "    angle_xz = np.random.uniform(min_angle_xz, max_angle_xz)\n",
    "    angle_yz = np.random.uniform(min_angle_yz, max_angle_yz)\n",
    "    rotation_axis = np.random.choice([0,1,2])\n",
    "    if(rotation_axis == 0):\n",
    "        X_im = ndimage.rotate(X_im, angle = angle_xy, axes = (0,1), mode = \"nearest\", reshape = False, order = 3)\n",
    "    if(rotation_axis == 1):\n",
    "        X_im = ndimage.rotate(X_im, angle = angle_xz, axes = (0,2), mode = \"nearest\", reshape = False, order = 3)\n",
    "    if(rotation_axis == 2):\n",
    "        X_im = ndimage.rotate(X_im, angle = angle_yz, axes = (1,2), mode = \"nearest\", reshape = False, order = 3)\n",
    "    return X_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shifting\n",
    "def random_shift3d(X_im, min_shift_x, max_shift_x, min_shift_y, max_shift_y, min_shift_z, max_shift_z):\n",
    "    x_shift = np.random.uniform(min_shift_x, max_shift_x)\n",
    "    y_shift = np.random.uniform(min_shift_y, max_shift_y)\n",
    "    z_shift = np.random.uniform(min_shift_z, max_shift_z)\n",
    "    return ndimage.shift(X_im, [x_shift, y_shift, z_shift, 0], mode = \"nearest\", order = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flip\n",
    "def random_flip3d(X_im):\n",
    "    axis = np.random.choice([0,1])\n",
    "    if(axis == 0): # vertical flip\n",
    "        X_im = X_im[:,::-1,:,:]\n",
    "    return X_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# smoothing\n",
    "def random_gaussianfilter3d(X_im, sigma_max):\n",
    "    sigma = np.random.uniform(0, sigma_max)\n",
    "    return ndimage.gaussian_filter(X_im, sigma, mode = \"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine augmentation functions:\n",
    "def augment_batch(X_batch):\n",
    "    X_batch_aug = np.empty_like(X_batch)\n",
    "    for i in range(X_batch.shape[0]):\n",
    "        im = X_batch[i]\n",
    "        im = random_zoom3d(im, 0.7, 1.4)\n",
    "        im = random_rotate3d(im, -30, 30, -10, 10, -10, 10)\n",
    "        im = random_shift3d(im, -20, 20, -20, 20, -5, 5)\n",
    "        im = random_flip3d(im)\n",
    "        im = random_gaussianfilter3d(im, 0.2)\n",
    "        X_batch_aug[i] = im\n",
    "    return X_batch_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train validation test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get TIA = 0 and stroke = 1 indices\n",
    "idx_0 = np.where(Y == 0)\n",
    "idx_1 = np.where(Y == 1)\n",
    "print(\"{} TIA patients\".format(len(idx_0[0])))\n",
    "print(\"{} stroke patients\".format(len(idx_1[0])))\n",
    "\n",
    "## shuffle indices\n",
    "np.random.seed(2021)\n",
    "np.random.shuffle(idx_0[0])\n",
    "np.random.shuffle(idx_1[0])\n",
    "\n",
    "## split indices into 5 parts\n",
    "splits_0 = np.array_split(idx_0[0], N_FOLDS)\n",
    "splits_1 = np.array_split(idx_1[0], N_FOLDS)\n",
    "\n",
    "## define chosen splits for each fold\n",
    "test_folds = [0, 1, 2, 3, 4]\n",
    "valid_folds = [1, 2, 3, 4, 0]\n",
    "train_folds = [[0, 1], [1, 2], [2, 3], [3, 4], [0, 4]] ## remove these splits for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models for image data\n",
    "### Complex intercept Linear Shift\n",
    "Train with\n",
    "- imaging and tabular data\n",
    "- imaging data as complex intercept\n",
    "- tabular data as linear shift\n",
    "- outcome = stroke\n",
    "- Ensemble with 5 Models\n",
    "- 5-Fold CV\n",
    "- \"Warmstart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear shift\n",
    "def linear_shift_x(x):\n",
    "    in_ = keras.Input(shape = x.shape[1:], name = 'x_in')\n",
    "    out_ = layers.Dense(1, activation = 'linear',\n",
    "                        use_bias = False, name = 'x_out')(in_)\n",
    "    nn_x = keras.Model(inputs = in_, outputs = out_)\n",
    "    return nn_x\n",
    "\n",
    "# complex shift for image\n",
    "def complex_intercept_b(input_shape, output_shape, input_name, activation = \"linear\"):\n",
    "    \n",
    "    initializer = keras.initializers.HeNormal(seed = 2802)\n",
    "    \n",
    "    in_ = keras.Input(shape = input_shape, name = input_name)\n",
    "    x = layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', \n",
    "                             activation = 'relu', kernel_initializer = initializer)(in_)\n",
    "    x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 1))(x) # evtl (2,2,2)\n",
    "    x = layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', \n",
    "                             activation = 'relu', kernel_initializer = initializer)(x)\n",
    "    x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', \n",
    "                             activation = 'relu', kernel_initializer = initializer)(x)\n",
    "    x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', \n",
    "                             activation = 'relu', kernel_initializer = initializer)(x)\n",
    "    x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = layers.Convolution3D(128, kernel_size=(3, 3, 3), padding = 'same', \n",
    "                             activation = 'relu', kernel_initializer = initializer)(x)\n",
    "    x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation = 'relu', kernel_initializer = initializer)(x)\n",
    "    x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation = 'relu', kernel_initializer = initializer)(x)\n",
    "    x = layers.BatchNormalization(center=True, scale=True)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    h = layers.Dense(output_shape, activation = activation)(x) # activation = linear\n",
    "    \n",
    "    out_ = layers.Lambda(lambda x: x * 0.1)(h) # get rid of too large values for h\n",
    "    \n",
    "    nn_im = keras.Model(inputs = in_, outputs = out_)\n",
    "    return nn_im\n",
    "\n",
    "logreg_model = linear_model.LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fold in range(N_FOLDS):\n",
    "    \n",
    "    ## define train, test and validation splits\n",
    "    test_idx = np.concatenate((splits_0[test_folds[fold]], splits_1[test_folds[fold]]), axis = None)\n",
    "    valid_idx = np.concatenate((splits_0[valid_folds[fold]], splits_1[valid_folds[fold]]), axis = None)\n",
    "\n",
    "    train_0 = np.delete(splits_0, train_folds[fold], 0)\n",
    "    train_0 = [item for sublist in train_0 for item in sublist]\n",
    "    \n",
    "    train_1 = np.delete(splits_1, train_folds[fold], 0)\n",
    "    train_1 = [item for sublist in train_1 for item in sublist]\n",
    "    \n",
    "    train_idx = np.concatenate((train_0, train_1), axis = None)\n",
    "    \n",
    "    X_im_train = X[train_idx]\n",
    "    X_im_test = X[test_idx]\n",
    "    X_im_valid = X[valid_idx]\n",
    "    \n",
    "    X_tab_train = X_tab[train_idx]\n",
    "    X_tab_test = X_tab[test_idx]\n",
    "    X_tab_valid = X_tab[valid_idx]\n",
    "    \n",
    "    Y_train = Y[train_idx]\n",
    "    Y_test = Y[test_idx]\n",
    "    Y_valid = Y[valid_idx] \n",
    "    \n",
    "    logreg_model.fit(X_tab_train, Y_train)\n",
    "    LSx_weights = logreg_model.coef_.reshape(12, 1)\n",
    "    \n",
    "    Y_train = to_categorical(Y_train)\n",
    "    Y_valid = to_categorical(Y_valid)\n",
    "    Y_test = to_categorical(Y_test)\n",
    "\n",
    "    for run in range(N_ENSEMBLES):\n",
    "    \n",
    "        ## create output directory\n",
    "        folder_name = \"CIb_LSx/fold_{}/run_{}/\".format(fold, run)\n",
    "        if not os.path.exists(OUTPUT_DIR + folder_name):\n",
    "            os.makedirs(OUTPUT_DIR + folder_name)\n",
    "       \n",
    "        print(\"training fold {}/{}, run {}/{}\".format(fold+1, N_FOLDS, run+1, N_ENSEMBLES))\n",
    "    \n",
    "        ## compile and fit model\n",
    "        nn_bl = complex_intercept_b(X_im_train.shape[1:], Y_train.shape[1]-1, \"bl_in\", \"linear\")\n",
    "        ## load weights from trained model in CIb, same fold, same run\n",
    "        nn_bl.load_weights('{}fold_{}/best_model_run{}.hdf5'.format(MODEL_DIR, fold, run))\n",
    "        \n",
    "        nn_x = linear_shift_x(X_tab_train)\n",
    "        ## set weights from logistic regression with corresponding fold + noise\n",
    "        np.random.seed(1234 + run)\n",
    "        noise = np.random.normal(loc = 0, scale = 0.1, size = 12)\n",
    "        nn_x.set_weights([np.add(LSx_weights.flatten(), noise).reshape(12, 1)])\n",
    "        \n",
    "        ci_ls = ontram(nn_bl = nn_bl, nn_x = nn_x, response_varying = True)\n",
    "        \n",
    "        hist = fit_ontram(ci_ls,\n",
    "                          y_train = Y_train,\n",
    "                          x_train = X_tab_train,\n",
    "                          x_train_im = X_im_train,\n",
    "                          x_test = X_tab_valid,\n",
    "                          x_test_im = X_im_valid, \n",
    "                          y_test = Y_valid,\n",
    "                          batch_size = 32,\n",
    "                          epochs = 200,\n",
    "                          optimizer = tf.keras.optimizers.Adam(lr = 0.0001),\n",
    "                          augment_batch = augment_batch,\n",
    "                          balance_batches = True,\n",
    "                          output_dir = OUTPUT_DIR + folder_name)\n",
    "\n",
    "        ## save training loss and accuracy\n",
    "        out = pd.DataFrame({'fold': fold,\n",
    "                            'run': run,\n",
    "                            'train_loss': hist[\"train_loss\"], \n",
    "                            'train_acc': hist[\"train_acc\"],\n",
    "                            'test_loss': hist[\"test_loss\"], \n",
    "                            'test_acc': hist[\"test_acc\"]})\n",
    "        if run == 0 and fold == 0:\n",
    "            out.to_csv(\"{}CIb_LSx/ensemble_history.csv\".format(OUTPUT_DIR), index = False)\n",
    "        else:\n",
    "            out.to_csv(\"{}CIb_LSx/ensemble_history.csv\".format(OUTPUT_DIR), \n",
    "                       mode='a', header=False, index = False)\n",
    "\n",
    "        ## save best model\n",
    "        best_model = np.where(out.test_loss == np.min(out.test_loss))[0][0]\n",
    "        print('best model run {}: {}'.format(run, best_model))\n",
    "        ci_ls.model.load_weights('{}{}model-{:03d}.hdf5'.format(OUTPUT_DIR, folder_name, best_model))\n",
    "        ci_ls.model.save_weights('{}CIb_LSx/fold_{}/best_model_run{}.hdf5'.format(OUTPUT_DIR, fold, run))\n",
    "        \n",
    "        # predict model\n",
    "        pred = predict(ci_ls, bl = X_im_test, x = X_tab_test, y = Y_test)\n",
    "        out = pd.DataFrame({'pid': np.array(pat)[test_idx],\n",
    "                            'fold': fold,\n",
    "                            'run': run,\n",
    "                            'pred_prob_tia': pred[\"pdf\"][:, 0],\n",
    "                            'pred_prob_stroke': pred[\"pdf\"][:, 1],\n",
    "                            'pred_label_stroke': pred[\"response\"],\n",
    "                            'patient_label_tia': Y_test[:, 0],\n",
    "                            'patient_label_stroke': Y_test[:, 1]})\n",
    "        if run == 0 and fold == 0:\n",
    "            out.to_csv(\"{}CIb_LSx/ensemble_predictions.csv\".format(OUTPUT_DIR), index = False)\n",
    "        else:\n",
    "            out.to_csv(\"{}CIb_LSx/ensemble_predictions.csv\".format(OUTPUT_DIR), \n",
    "                       mode='a', header=False, index = False)\n",
    "            \n",
    "        ## save model weights\n",
    "        names = ['age', 'sex', 'mrs_before', 'nihss_baseline', 'stroke_before', \n",
    "                     'tia_before', 'rf_hypertonia', 'rf_diabetes', 'rf_hypercholesterolemia', \n",
    "                     'rf_smoker', 'rf_atrial_fibrillation', 'rf_chd']\n",
    "        weights = np.array(pred['beta_w']).flatten()\n",
    "        out = pd.DataFrame({'fold': fold,\n",
    "                            'run': run,\n",
    "                            'names': names,\n",
    "                            'coef': weights})\n",
    "        if run == 0 and fold == 0:\n",
    "            out.to_csv(\"{}CIb_LSx/ensemble_weights.csv\".format(OUTPUT_DIR), index = False)\n",
    "        else:\n",
    "            out.to_csv(\"{}CIb_LSx/ensemble_weights.csv\".format(OUTPUT_DIR), \n",
    "                       mode='a', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = pd.read_csv(\"{}CIb_LSx/ensemble_predictions.csv\".format(OUTPUT_DIR))\n",
    "pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = pd.read_csv(\"{}CIb_LSx/ensemble_weights.csv\".format(OUTPUT_DIR))\n",
    "weights.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.read_csv(\"{}CIb_LSx/ensemble_history.csv\".format(OUTPUT_DIR))\n",
    "hist.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
